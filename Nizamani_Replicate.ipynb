{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Librarairs\n",
    "import os,json\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "CWD = os.getcwd()  # get current directory path\n",
    "BUG_REPORTS_DIR = os.path.join(CWD,\"bugs\")   # current directory+bugs\n",
    "BUG_REPORTS_LIST = os.listdir(BUG_REPORTS_DIR)  # all the items in 'current_directory+bugs'\n",
    "\n",
    "SENTI_REPORTS_DIR = os.path.join(CWD,\"Sentiments_output\")\n",
    "SENTI_REPORTS_LIST = os.listdir(SENTI_REPORTS_DIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cr/Desktop/bugs\n",
      "-----------------\n",
      "['Core.json', 'Toolkit.json', 'bugzilla.json', 'Calendar.json', 'SeaMonkey.json', 'Camino Graveyard.json', 'MailNews Core.json', 'Firefox.json', 'Thunderbird.json', 'Core Graveyard.json']\n",
      "-----------------\n",
      "/home/cr/Desktop/Sentiments_output\n",
      "-----------------\n",
      "['Camino_Graveyard_sentiments.csv', 'bugzilla_sentiments.csv', 'Firefox_sentiments.csv', 'Core_sentiments.csv', 'MailNews_Core_sentiments.csv', 'SeaMonkey_sentiments.csv', 'Thunderbird_sentiments.csv', 'Toolkit_sentiments.csv', 'Core_Graveyard_sentiments.csv', 'Calendar_sentiments.csv']\n"
     ]
    }
   ],
   "source": [
    "# Load data set for bugs and sentiments\n",
    "\n",
    "print(BUG_REPORTS_DIR)\n",
    "print('-----------------')\n",
    "print(BUG_REPORTS_LIST)\n",
    "print('-----------------')\n",
    "print(SENTI_REPORTS_DIR)\n",
    "print('-----------------')\n",
    "print(SENTI_REPORTS_LIST)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set: Camino_Graveyard_sentiments.csv\n",
      "1291\n",
      "Train Set: bugzilla_sentiments.csv\n",
      "5325\n",
      "Train Set: Firefox_sentiments.csv\n",
      "7369\n",
      "Train Set: Core_sentiments.csv\n",
      "7229\n",
      "Train Set: MailNews_Core_sentiments.csv\n",
      "2797\n",
      "Train Set: SeaMonkey_sentiments.csv\n",
      "8717\n",
      "Test Set: Thunderbird_sentiments.csv\n",
      "8717\n",
      "Train Set: Toolkit_sentiments.csv\n",
      "1900\n",
      "Train Set: Core_Graveyard_sentiments.csv\n",
      "889\n",
      "Train Set: Calendar_sentiments.csv\n",
      "1786\n"
     ]
    }
   ],
   "source": [
    "# Extract data from files for sentiments\n",
    "import csv\n",
    "test_senti = []\n",
    "train_senti=[]\n",
    "import pandas as pd\n",
    "for count, report in enumerate(SENTI_REPORTS_LIST):\n",
    "    \n",
    "    if count == 6:\n",
    "        #add this project to test set\n",
    "        print(f'Test Set: {report}')\n",
    "        with open(os.path.join(SENTI_REPORTS_DIR,report), 'r') as report_file:\n",
    "            temp_data = pd.read_csv(report_file)\n",
    "            print(len(temp_data))\n",
    "            #print(temp_data['neutral'])\n",
    "            test_senti.extend(temp_data.iloc[0:, 1])\n",
    "            \n",
    "    else:\n",
    "        #appent this project to train set\n",
    "        print(f'Train Set: {report}')\n",
    "        with open(os.path.join(SENTI_REPORTS_DIR,report), 'r',encoding='cp850') as report_file:\n",
    "            temp_data = pd.read_csv(report_file)\n",
    "            print(len(temp_data))\n",
    "            #print(temp_data.iloc[0:, 1])\n",
    "            train_senti.extend(temp_data.iloc[0:, 1])\n",
    "            \n",
    "            #print(f\"File:{report} <> Reports: {len(temp_data['bugs'])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set: Core.json\n",
      "File:Core.json <> Reports: 7228\n",
      "Train Set: Toolkit.json\n",
      "File:Toolkit.json <> Reports: 1899\n",
      "Train Set: bugzilla.json\n",
      "File:bugzilla.json <> Reports: 5324\n",
      "Train Set: Calendar.json\n",
      "File:Calendar.json <> Reports: 1785\n",
      "Train Set: SeaMonkey.json\n",
      "File:SeaMonkey.json <> Reports: 8716\n",
      "Train Set: Camino Graveyard.json\n",
      "File:Camino Graveyard.json <> Reports: 1290\n",
      "Train Set: MailNews Core.json\n",
      "File:MailNews Core.json <> Reports: 2796\n",
      "Train Set: Firefox.json\n",
      "File:Firefox.json <> Reports: 7368\n",
      "Train Set: Thunderbird.json\n",
      "File:Thunderbird.json <> Reports: 5129\n",
      "Test Set: Core Graveyard.json\n",
      "File:Core Graveyard.json <> Reports: 888\n"
     ]
    }
   ],
   "source": [
    "# Load data for bugs \n",
    "train_set = []\n",
    "test_set = []\n",
    "\n",
    "for count, report in enumerate(BUG_REPORTS_LIST):\n",
    "    if count == 9:\n",
    "        #add this project to test set\n",
    "        print(f'Test Set: {report}')\n",
    "        with open(os.path.join(BUG_REPORTS_DIR,report), 'r',encoding='cp850') as report_file:\n",
    "            temp_data = json.load(report_file)\n",
    "            print(f\"File:{report} <> Reports: {len(temp_data['bugs'])}\")\n",
    "            test_set.extend(temp_data[\"bugs\"])\n",
    "    else:\n",
    "        #appent this project to train set\n",
    "        print(f'Train Set: {report}')\n",
    "        with open(os.path.join(BUG_REPORTS_DIR,report), 'r',encoding='cp850') as report_file:\n",
    "            temp_data = json.load(report_file)\n",
    "            print(f\"File:{report} <> Reports: {len(temp_data['bugs'])}\")\n",
    "            train_set.extend(temp_data[\"bugs\"])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train len: 41535\n",
      "Test len: 888\n",
      "Total len: 42423\n"
     ]
    }
   ],
   "source": [
    "print(f'Train len: {len(train_set)}')\n",
    "print(f'Test len: {len(test_set)}')\n",
    "print(f'Total len: {len(train_set)+len(test_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_summary 41535\n",
      "train_label 41535\n",
      "test_summary 888\n",
      "test_label 888\n"
     ]
    }
   ],
   "source": [
    "summary_train=[]\n",
    "label_train=[]\n",
    "summary_test=[]\n",
    "label_test=[]\n",
    "for i in train_set:\n",
    "    summary_train.append(i['summary'])\n",
    "    #summary_train=summary_train[0:10]\n",
    "    label_train.append(i['resolution'])\n",
    "print('train_summary',len(summary_train))\n",
    "print('train_label',len(label_train))\n",
    "\n",
    "    #label_train=label_train[0:10]\n",
    "for j in test_set:\n",
    "    summary_test.append(j['summary'])\n",
    "    #summary_test=summary_test[0:10]\n",
    "    label_test.append(j['resolution'])\n",
    "   # label_test=label_test[0:10]\n",
    "print('test_summary',len(summary_test))\n",
    "print('test_label',len(label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def senti_to_int(train_test_senti):\n",
    "    \n",
    "    train_test_sentiments=[]\n",
    "    for x in range(len(train_test_senti)):\n",
    "        if train_test_senti[x]==\"positive\":\n",
    "            train_test_senti[x]=1\n",
    "        elif train_test_senti[x]==\"neutral\":\n",
    "            train_test_senti[x]=1\n",
    "        elif train_senti[x]==\"negative\":\n",
    "            train_test_senti[x]=0\n",
    "        train_test_sentiments.append(train_test_senti[x])\n",
    "    return train_test_sentiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sentimetns Length:37303\n",
      "Testing sentiments Length:8717\n"
     ]
    }
   ],
   "source": [
    "train_sentiment=senti_to_int(train_senti)\n",
    "test_sentiment=senti_to_int(test_senti)\n",
    "print(f\"Training sentimetns Length:{len(train_sentiment)}\")\n",
    "print(f\"Testing sentiments Length:{len(test_sentiment)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiments \n",
    "\n",
    "train_senti=np.array(train_sentiment)\n",
    "test_senti=np.array(test_sentiment)\n",
    "#train_sentiments.shape\n",
    "train_sentiments=train_senti.reshape(len(train_senti),1)\n",
    "test_sentiments=test_senti.reshape(len(test_senti),1)\n",
    "\n",
    "\n",
    "##  use \n",
    "# train_sentiments\n",
    "# test_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train len: 37303\n",
      "Test len: 8717\n",
      "Total len: 46020\n",
      "(8717, 1)\n",
      "(37303, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f'Train len: {len(train_sentiments)}')\n",
    "print(f'Test len: {len(test_sentiments)}')\n",
    "print(f'Total len: {len(train_sentiments)+len(test_sentiments)}')\n",
    "print(test_sentiments.shape)\n",
    "print(train_sentiments.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess sentences\n",
    "import re\n",
    "def train_test_summary(summaries_text):\n",
    "    \n",
    "    lemmas=[]\n",
    "    lemmatized_word=[]\n",
    "    train_test_summaries=[]\n",
    "    for count, summaries in enumerate (summaries_text):\n",
    "        text = re.sub(r'[^a-zA-Z\\']', ' ', summaries)\n",
    "        #removes all single characters\n",
    "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "        #substitute multiple spcaces with single space\n",
    "        s_m_s = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "        #removes prefix\n",
    "        r_prefix = re.sub(r'^b\\s+', '', s_m_s)\n",
    "        #lemmas.append(r_prefix)\n",
    "        r_pm = re.sub(r'[^\\w\\s]', '', r_prefix)\n",
    "        step1 = r_pm.replace(\"`` \", '\"').replace(\" ''\", '\"').replace('. . .',  '...')\n",
    "        step2 = step1.replace(\" ( \", \" (\").replace(\" ) \", \") \")\n",
    "        step3 = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", step2)\n",
    "        step4 = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", step3)\n",
    "        step5 = step4.replace(\" '\", \"'\").replace(\" n't\", \"n't\").replace(\n",
    "        \"can not\", \"cannot\")\n",
    "        step6 = step5.replace(\" ` \", \" '\")\n",
    "        step7=step6.strip()\n",
    "        step8=step7.lower()    \n",
    "        lemmas=[]\n",
    "        sentences = nltk.word_tokenize(step7)\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        stem_words=[]\n",
    "        for sentence_words in sentences:\n",
    "            lower=sentence_words.lower()\n",
    "            if lower not in stopwords.words('english'):\n",
    "                lemma = lemmatizer.lemmatize(lower)\n",
    "                stem_words.append(lemma)\n",
    "            untokenized_sentence= ' '.join(stem_words)\n",
    "        train_test_summaries.append(untokenized_sentence)\n",
    "    return train_test_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encocede Labels \n",
    "def train_test_labels(labels):\n",
    "    train_test_label=[]\n",
    "    for x in range(len(labels)):\n",
    "        \n",
    "        \n",
    "        if label_train[x]==\"FIXED\":\n",
    "            label_train[x]=1\n",
    "        elif label_train[x]==\"WONTFIX\":\n",
    "            label_train[x]=0\n",
    "        elif label_train[x]==\"INVALID\":\n",
    "            label_train[x]=0\n",
    "        elif label_train[x]==\"INCOMPLETE\":\n",
    "            label_train[x]=0\n",
    "        elif label_train[x]==\"WONTFIX\":\n",
    "            label_train[x]=0\n",
    "        elif label_train[x]==\"WORKSFORME\":\n",
    "            label_train[x]=0\n",
    "        elif label_train[x]==\"EXPIRED\":\n",
    "            label_train[x]=0\n",
    "        elif label_train[x]==\"DUPLICATE\":\n",
    "            label_train[x]=0\n",
    "        elif label_train[x]==\"\":\n",
    "            label_train[x]=0\n",
    "        train_test_label.append(label_train[x])\n",
    "        \n",
    "    \n",
    "    \n",
    "    #print('train_lbel', len(train_label))\n",
    "    return train_test_label\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Labels Length:41535\n",
      "Testing Labels Length:888\n"
     ]
    }
   ],
   "source": [
    "train_labels= train_test_labels(label_train)\n",
    "test_labels=train_test_labels(label_test)\n",
    "print(f\"Training Labels Length:{len(train_labels)}\")\n",
    "print(f\"Testing Labels Length:{len(test_labels)}\")\n",
    "\n",
    "\n",
    "# Use \n",
    "#     train_labels\n",
    "#     test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training summary Length:41535\n",
      "Testing summary Length:888\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_summary=train_test_summary(summary_train)\n",
    "test_summary=train_test_summary(summary_test)\n",
    "\n",
    "print(f\"Training summary Length:{len(train_summary)}\")\n",
    "print(f\"Testing summary Length:{len(test_summary)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Use \n",
    "#     train_summary\n",
    "#     test_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature cs first letter selector isnt finished\n"
     ]
    }
   ],
   "source": [
    "print(train_summary[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert Labels (integers) to numpy array for feeding to a model and for evaluation\n",
    "\n",
    "train_labels=np.array(train_labels)\n",
    "test_labels=np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Libraries\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#from gensim.models import doc2vec\n",
    "from collections import namedtuple\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41535, 3000)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BOW\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(max_features=3000, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X_train_counts = count_vect.fit_transform(train_summary)\n",
    "X_train_counts.shape\n",
    "\n",
    "\n",
    "# here shape shows rows and colums, colums are according to feature number, total no.of features=total no. of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('add', 3894), ('tab', 3142), ('rfe', 2712), ('message', 2695), ('window', 2581), ('new', 2522), ('option', 2397), ('mail', 2357), ('page', 1966), ('folder', 1942)]\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer().fit(train_summary)\n",
    "bag_of_words = vec.transform(train_summary)\n",
    "sum_words = bag_of_words.sum(axis=0) \n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n",
    "words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "print(words_freq[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "b=vec.vocabulary_.get(u'should')\n",
    "print(b)\n",
    "\n",
    "#The index value of a word in the vocabulary is linked to its frequency in the whole training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41535, 3000)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41535, 3000)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_counts = count_vect.transform(test_summary)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.96      0.80       606\n",
      "           1       0.35      0.05      0.09       282\n",
      "\n",
      "    accuracy                           0.67       888\n",
      "   macro avg       0.52      0.50      0.44       888\n",
      "weighted avg       0.58      0.67      0.57       888\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#MUltinomail_naive Bayes\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, train_labels)\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "print(metrics.classification_report(test_labels, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cr/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      1.00      0.79      4754\n",
      "           1       0.00      0.00      0.00      2474\n",
      "\n",
      "    accuracy                           0.66      7228\n",
      "   macro avg       0.33      0.50      0.40      7228\n",
      "weighted avg       0.43      0.66      0.52      7228\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cr/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "from sklearn import svm\n",
    "clf = svm.SVC()\n",
    "svm_clf=clf.fit(X_train_tfidf, train_label)\n",
    "SVC()\n",
    "predictions=svm_clf.predict(X_new_tfidf)\n",
    "print(metrics.classification_report(test_label, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      1.00      0.79      4754\n",
      "           1       1.00      0.00      0.00      2474\n",
      "\n",
      "    accuracy                           0.66      7228\n",
      "   macro avg       0.83      0.50      0.40      7228\n",
      "weighted avg       0.77      0.66      0.52      7228\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_text\n",
    "decision_tree = DecisionTreeClassifier(random_state=0, max_depth=2)\n",
    "decision_tree = decision_tree.fit(X_train_tfidf, train_label)\n",
    "prediction=decision_tree.predict(X_new_tfidf)\n",
    "print(metrics.classification_report(test_label, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.97      0.79      4754\n",
      "           1       0.40      0.03      0.06      2474\n",
      "\n",
      "    accuracy                           0.65      7228\n",
      "   macro avg       0.53      0.50      0.42      7228\n",
      "weighted avg       0.57      0.65      0.54      7228\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier.fit(X_train_tfidf, train_label) \n",
    "y_pred = classifier.predict(X_new_tfidf)\n",
    "print(metrics.classification_report(test_label, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Term frequency = (Number of Occurrences of a word)/(Total words in the document)\n",
    "#IDF(word) = Log((Total number of documents)/(Number of documents containing the word))\n",
    "\n",
    "\n",
    "# There are two ways to create TF_IDF embeddings \n",
    "# 1. make vecrors first using COuntVectorizer to generate one hot encoding vectors BOW (1,0 if exist,then 1. otherwise 0)\n",
    "# and then transform it into TF-IDF using TF-IDF transformer \n",
    "# 2. Directly use TF-IDF vectorizer to generate TF-IDF embeddings to feed a model \n",
    "\n",
    "# we can use here uni_gram, bi-Gram and tri_gram embedding also, then it will generate differnt embeddings.\n",
    "\n",
    "# 1st method\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=10000, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(train_summary).toarray()\n",
    "\n",
    "# till here it is simple embedding into 0 if words doesn't exist, and n=no.of times a word occurs\n",
    "# documents: #1. ali go go to school, #2. go to shooingmall, #3. takes rest , #4. go to sleeop\n",
    "# features set: 1. ali, 2. go, 3. school, 4. shoppingmall, 5. take, 6. rest, 7. sleep\n",
    "# embedding dimensions(3,7)  # Rows = no.of documents & columns = no.of features   \n",
    "# embedding for document #1 = [1,2,1,0,0,0,0]\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "TfidfTransformer = TfidfTransformer()\n",
    "X_train= TfidfTransformer.fit_transform(X).toarray()\n",
    "X_test=vectorizer.transform(test_summary).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'TfidfTransformer' object is not callable",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-534-081a8a58da57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m text_clf = Pipeline([\n\u001b[1;32m      7\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'vect'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0;34m'tfidf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTfidfTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'clf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m ])\n",
      "\u001b[0;31mTypeError\u001b[0m: 'TfidfTransformer' object is not callable"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#In order to make the vectorizer => transformer => classifier easier to work with, \n",
    "#scikit-learn provides a Pipeline class that behaves like a compound classifier:\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "text_clf = Pipeline([\n",
    "('vect', CountVectorizer()),\n",
    "('tfidf', TfidfTransformer()),\n",
    "('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_clf.fit(X_train, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.85      0.75      4754\n",
      "           1       0.41      0.20      0.27      2474\n",
      "\n",
      "    accuracy                           0.63      7228\n",
      "   macro avg       0.54      0.53      0.51      7228\n",
      "weighted avg       0.58      0.63      0.59      7228\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train, train_label)\n",
    "predicted = clf.predict(X_test)\n",
    "print(metrics.classification_report(test_label, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      1.00      0.79      4754\n",
      "           1       1.00      0.00      0.00      2474\n",
      "\n",
      "    accuracy                           0.66      7228\n",
      "   macro avg       0.83      0.50      0.40      7228\n",
      "weighted avg       0.77      0.66      0.52      7228\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_text\n",
    "decision_tree = DecisionTreeClassifier(random_state=0, max_depth=2)\n",
    "decision_tree = decision_tree.fit(X_train, train_label)\n",
    "prediction=decision_tree.predict(X_test)\n",
    "print(metrics.classification_report(test_label, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "                            # just BOW Approach # Used By QASIM\n",
    "\n",
    "    \n",
    "    \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=2000, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(train_summary).toarray()\n",
    "Y=vectorizer.transform(test_summary).toarray()\n",
    "X=np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Senitments Appended\n",
    "\n",
    "senti_summary_train= np.append(X,train_sentiments, axis=1)\n",
    "senti_summary_test= np.append(Y,test_sentiments, axis=1)\n",
    "X1=senti_summary_train\n",
    "Y1=senti_summary_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.84      0.75      4754\n",
      "           1       0.41      0.22      0.29      2474\n",
      "\n",
      "    accuracy                           0.63      7228\n",
      "   macro avg       0.54      0.53      0.52      7228\n",
      "weighted avg       0.58      0.63      0.59      7228\n",
      "\n",
      "roc 0.5285217843188003\n",
      "MATHEOS_COEFFICIENT 0.06997114109487436\n",
      "ODDS RATIO [[5.50372878e-05 3.16464405e-03 1.81623050e-03 ... 5.50372878e-05\n",
      "  7.70522029e-04 8.25559316e-05]]\n",
      "CONFUSION MATRIX [[3976  778]\n",
      " [1928  546]]\n"
     ]
    }
   ],
   "source": [
    "# 1. MNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import scipy.stats as stats\n",
    "clf = MultinomialNB().fit(X,train_labels)\n",
    "predicted = clf.predict(Y)\n",
    "print(metrics.classification_report(test_labels,predicted))\n",
    "\n",
    "\n",
    "roc=roc_auc_score(test_labels,predicted)\n",
    "print('roc',roc)\n",
    "metheows=matthews_corrcoef(test_labels,predicted)\n",
    "print('MATHEOS_COEFFICIENT',metheows)\n",
    "odds_ratio=np.exp(clf.coef_)\n",
    "print('ODDS RATIO',odds_ratio)\n",
    "matrix = metrics.confusion_matrix(test_labels,predicted)\n",
    "print('CONFUSION MATRIX',matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      1.00      0.79      4754\n",
      "           1       1.00      0.00      0.00      2474\n",
      "\n",
      "    accuracy                           0.66      7228\n",
      "   macro avg       0.83      0.50      0.40      7228\n",
      "weighted avg       0.77      0.66      0.52      7228\n",
      "\n",
      "roc 0.5002021018593371\n",
      "MATHEOS_COEFFICIENT 0.016306122989834953\n",
      "ODDS RATIO [[5.50372878e-05 3.16464405e-03 1.81623050e-03 ... 5.50372878e-05\n",
      "  7.70522029e-04 8.25559316e-05]]\n",
      "CONFUSION MATRIX [[4754    0]\n",
      " [2473    1]]\n"
     ]
    }
   ],
   "source": [
    "# 2.  Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_text\n",
    "decision_tree = DecisionTreeClassifier(random_state=0, max_depth=2)\n",
    "decision_tree = decision_tree.fit(X, train_labels)\n",
    "prediction=decision_tree.predict(Y)\n",
    "print(metrics.classification_report(test_labels, prediction))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "roc=roc_auc_score(test_labels, prediction)\n",
    "print('roc',roc)\n",
    "metheows=matthews_corrcoef(test_labels, prediction)\n",
    "print('MATHEOS_COEFFICIENT',metheows)\n",
    "odds_ratio=np.exp(clf.coef_)\n",
    "print('ODDS RATIO',odds_ratio)\n",
    "matrix = metrics.confusion_matrix(test_labels, prediction)\n",
    "print('CONFUSION MATRIX',matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.94      0.78      4754\n",
      "           1       0.43      0.09      0.15      2474\n",
      "\n",
      "    accuracy                           0.65      7228\n",
      "   macro avg       0.55      0.51      0.46      7228\n",
      "weighted avg       0.58      0.65      0.56      7228\n",
      "\n",
      "roc 0.5139617780066245\n",
      "MATHEOS_COEFFICIENT 0.05173626501844653\n",
      "ODDS RATIO [[5.50372878e-05 3.16464405e-03 1.81623050e-03 ... 5.50372878e-05\n",
      "  7.70522029e-04 8.25559316e-05]]\n",
      "CONFUSION MATRIX [[4464  290]\n",
      " [2254  220]]\n"
     ]
    }
   ],
   "source": [
    "# 3. Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier.fit(X, train_labels) \n",
    "y_pred = classifier.predict(Y)\n",
    "print(metrics.classification_report(test_labels, y_pred))\n",
    "\n",
    "roc=roc_auc_score(test_labels, y_pred)\n",
    "print('roc',roc)\n",
    "metheows=matthews_corrcoef(test_labels, y_pred)\n",
    "print('MATHEOS_COEFFICIENT',metheows)\n",
    "odds_ratio=np.exp(clf.coef_)\n",
    "print('ODDS RATIO',odds_ratio)\n",
    "matrix = metrics.confusion_matrix(test_labels, y_pred)\n",
    "print('CONFUSION MATRIX',matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.95      0.78      4754\n",
      "           1       0.38      0.06      0.11      2474\n",
      "\n",
      "    accuracy                           0.64      7228\n",
      "   macro avg       0.52      0.51      0.44      7228\n",
      "weighted avg       0.57      0.64      0.55      7228\n",
      "\n",
      "roc 0.5053229225510305\n",
      "MATHEOS_COEFFICIENT 0.021811825836576235\n",
      "ODDS RATIO [[5.50372878e-05 3.16464405e-03 1.81623050e-03 ... 5.50372878e-05\n",
      "  7.70522029e-04 8.25559316e-05]]\n",
      "CONFUSION MATRIX [[4501  253]\n",
      " [2316  158]]\n"
     ]
    }
   ],
   "source": [
    "# 4. Logistic Regression \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf_lr = LogisticRegression(random_state=0).fit(X, train_labels)\n",
    "predicted = clf_lr.predict(Y)\n",
    "print(metrics.classification_report(test_labels, predicted))\n",
    "\n",
    "roc=roc_auc_score(test_labels, predicted)\n",
    "print('roc',roc)\n",
    "metheows=matthews_corrcoef(test_labels, predicted)\n",
    "print('MATHEOS_COEFFICIENT',metheows)\n",
    "odds_ratio=np.exp(clf.coef_)\n",
    "print('ODDS RATIO',odds_ratio)\n",
    "matrix = metrics.confusion_matrix(test_labels, predicted)\n",
    "print('CONFUSION MATRIX',matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.85      0.75      4754\n",
      "           1       0.41      0.20      0.27      2474\n",
      "\n",
      "    accuracy                           0.63      7228\n",
      "   macro avg       0.54      0.53      0.51      7228\n",
      "weighted avg       0.58      0.63      0.59      7228\n",
      "\n",
      "roc 0.5267089893070517\n",
      "MATHEOS_COEFFICIENT 0.06809179559983118\n",
      "ODDS RATIO [[5.50372878e-05 3.16464405e-03 1.81623050e-03 ... 5.50372878e-05\n",
      "  7.70522029e-04 8.25559316e-05]]\n",
      "CONFUSION MATRIX [[4051  703]\n",
      " [1976  498]]\n"
     ]
    }
   ],
   "source": [
    "# 5. Bernouli Naive Bayes\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf_bn = BernoulliNB()\n",
    "clf_bn.fit(X,train_labels)\n",
    "predict=clf_bn.predict(Y)\n",
    "print(metrics.classification_report(test_labels, predict))\n",
    "\n",
    "roc=roc_auc_score(test_labels, predict)\n",
    "print('roc',roc)\n",
    "metheows=matthews_corrcoef(test_labels, predict)\n",
    "print('MATHEOS_COEFFICIENT',metheows)\n",
    "odds_ratio=np.exp(clf.coef_)\n",
    "print('ODDS RATIO',odds_ratio)\n",
    "matrix = metrics.confusion_matrix(test_labels, predict)\n",
    "print('CONFUSION MATRIX',matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.82      0.74      4754\n",
      "           1       0.39      0.21      0.27      2474\n",
      "\n",
      "    accuracy                           0.61      7228\n",
      "   macro avg       0.53      0.52      0.51      7228\n",
      "weighted avg       0.57      0.61      0.58      7228\n",
      "\n",
      "roc 0.5183713736022492\n",
      "MATHEOS_COEFFICIENT 0.04454252047934634\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MLPClassifier' object has no attribute 'coef_'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-de0b6cb583c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmetheows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmatthews_corrcoef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredcitons\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MATHEOS_COEFFICIENT'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetheows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0modds_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ODDS RATIO'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0modds_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredcitons\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MLPClassifier' object has no attribute 'coef_'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# 6. MLP \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5 , hidden_layer_sizes=(5, 2), random_state=1)\n",
    "clf.fit(X,train_labels)\n",
    "MLPClassifier(alpha=1e-05, hidden_layer_sizes=(5, 2), random_state=1,solver='lbfgs')\n",
    "predcitons=clf.predict(Y)\n",
    "print(metrics.classification_report(test_labels, predcitons))\n",
    "\n",
    "roc=roc_auc_score(test_labels, predcitons)\n",
    "print('roc',roc)\n",
    "metheows=matthews_corrcoef(test_labels, predcitons)\n",
    "print('MATHEOS_COEFFICIENT',metheows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION MATRIX [[3916  838]\n",
      " [1947  527]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MLPClassifier' object has no attribute 'coef_'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-45eeec604cc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredcitons\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CONFUSION MATRIX'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0modds_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ODDS RATIO'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0modds_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MLPClassifier' object has no attribute 'coef_'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "matrix = metrics.confusion_matrix(test_labels, predcitons)\n",
    "print('CONFUSION MATRIX',matrix)\n",
    "odds_ratio=np.exp(clf.coef_)\n",
    "print('ODDS RATIO',odds_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_roc_curve' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-2f393a2da8f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mfper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mplot_roc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mroc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_roc_curve' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# 07. KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier()  \n",
    "model.fit(X,train_labels)\n",
    "\n",
    "probs = model.predict_proba(Y)  \n",
    "probs = probs[:, 1]  \n",
    "fper, tper, thresholds = roc_curve(test_labels, probs) \n",
    "plot_roc_curve(fper, tper)\n",
    "print(metrics.classification_report(test_labels, probs))\n",
    "roc=roc_auc_score(test_labels, probs)\n",
    "print('roc',roc)\n",
    "metheows=matthews_corrcoef(test_labels, probs)\n",
    "print('MATHEOS_COEFFICIENT',metheows)\n",
    "odds_ratio=np.exp(clf.coef_)\n",
    "print('ODDS RATIO',odds_ratio)\n",
    "matrix = metrics.confusion_matrix(test_labels, probs)\n",
    "print('CONFUSION MATRIX',matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cr/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#7. SVM\n",
    "from sklearn import svm\n",
    "clf = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "svm_clf=clf.fit(X, train_labels)\n",
    "SVC()\n",
    "predictions=svm_clf.predict(Y)\n",
    "print(metrics.classification_report(test_labels, predictions))\n",
    "\n",
    "\n",
    "roc=roc_auc_score(test_labels, predictions)\n",
    "print('roc',roc)\n",
    "metheows=matthews_corrcoef(test_labels, predictions)\n",
    "print('MATHEOS_COEFFICIENT',metheows)\n",
    "odds_ratio=np.exp(clf.coef_)\n",
    "print('ODDS RATIO',odds_ratio)\n",
    "matrix = metrics.confusion_matrix(test_labels, predictions)\n",
    "print('CONFUSION MATRIX',matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                # TF-IDF used by Nizamani \n",
    "# 2nd method\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfconverter = TfidfVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X = tfidfconverter.fit_transform(train_summary).toarray()\n",
    "Y=tfidfconverter.transform(test_summary).toarray()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# max_features=no. of max_time_occuring_features, you want to use to feed into the model\n",
    "# min_df= minimimum document frequency, select the word that occurs in atleat 5 documents(summaris)\n",
    "# max_df= maximum document frequency, select the word that occurs not more than 70% of the documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input has n_features=13549 while the model has been trained with n_features=3000",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-e20d91136638>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_train_counts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcount_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_summary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mX_new_counts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcount_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_summary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX_new_tfidf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtfidf_transformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_new_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1317\u001b[0m                 raise ValueError(\"Input has n_features=%d while the model\"\n\u001b[1;32m   1318\u001b[0m                                  \" has been trained with n_features=%d\" % (\n\u001b[0;32m-> 1319\u001b[0;31m                                      n_features, expected_n_features))\n\u001b[0m\u001b[1;32m   1320\u001b[0m             \u001b[0;31m# *= doesn't work\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_idf_diag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input has n_features=13549 while the model has been trained with n_features=3000"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "count_vect= TfidfVectorizer()\n",
    "X_train_counts=count_vect.fit_transform(train_summary)\n",
    "X_new_counts=count_vect.transform(test_summary)\n",
    "X_new_tfidf=tfidf_transformer.transform(X_new_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. MNB \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X, train_labels)\n",
    "predicted = clf.predict(Y)\n",
    "print(metrics.classification_report(test_labels, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.97      0.80       606\n",
      "           1       0.38      0.05      0.08       282\n",
      "\n",
      "    accuracy                           0.67       888\n",
      "   macro avg       0.53      0.51      0.44       888\n",
      "weighted avg       0.59      0.67      0.57       888\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. MNB \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X, train_labels)\n",
    "predicted = clf.predict(Y)\n",
    "print(metrics.classification_report(test_labels, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. MNB \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X, train_labels)\n",
    "predicted = clf.predict(Y)\n",
    "print(metrics.classification_report(test_labels, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.  Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_text\n",
    "decision_tree = DecisionTreeClassifier(random_state=0, max_depth=2)\n",
    "decision_tree = decision_tree.fit(X, train_label)\n",
    "prediction=decision_tree.predict(Y)\n",
    "print(metrics.classification_report(test_label, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier.fit(X, train_label) \n",
    "y_pred = classifier.predict(Y)\n",
    "print(metrics.classification_report(test_label, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Logistic Regression \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf_lr = LogisticRegression(random_state=0).fit(X, train_label)\n",
    "predicted = clf_lr.predict(Y)\n",
    "print(metrics.classification_report(test_label, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Bernouli Naive Bayes\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf_bn = BernoulliNB()\n",
    "clf_bn.fit(X, train_label)\n",
    "predict=clf_bn.predict(Y)\n",
    "print(metrics.classification_report(test_label, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cr/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#6. SVM\n",
    "from sklearn import svm\n",
    "clf = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "svm_clf=clf.fit(X, train_label)\n",
    "SVC()\n",
    "predictions=svm_clf.predict(Y)\n",
    "print(metrics.classification_report(test_label, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#6 again SVM\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(X, train_label)\n",
    "predictions=SVM.predict(Y)\n",
    "print(metrics.classification_report(test_label, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(train_summary)\n",
    "X_train_counts.shape\n",
    "count_vect.vocabulary_.get(u'algorithm')\n",
    "\n",
    "#The index value of a word in the vocabulary is linked to its frequency in the whole training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1989)\t1\n",
      "  (0, 4387)\t1\n",
      "  (0, 6676)\t1\n",
      "  (0, 7952)\t1\n",
      "  (0, 8150)\t1\n",
      "  (0, 8461)\t1\n",
      "  (0, 10196)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X_train_counts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35195\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(train_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35195, 10588)"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From occurrences to frequencies¶\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1989)\t0.3779644730092272\n",
      "  (0, 4387)\t0.3779644730092272\n",
      "  (0, 6676)\t0.3779644730092272\n",
      "  (0, 7952)\t0.3779644730092272\n",
      "  (0, 8150)\t0.3779644730092272\n",
      "  (0, 8461)\t0.3779644730092272\n",
      "  (0, 10196)\t0.3779644730092272\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35195, 10588)"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 10196)\t0.49374309343903616\n",
      "  (0, 8461)\t0.536619147489694\n",
      "  (0, 8150)\t0.3389371690811846\n",
      "  (0, 7952)\t0.20724042931785985\n",
      "  (0, 6676)\t0.2908051104736087\n",
      "  (0, 4387)\t0.28607161155816424\n",
      "  (0, 1989)\t0.3795076664350987\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tfidf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([\n",
    "     ('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', MultinomialNB()),\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_clf.fit(X_train_tfidf,train_labels)\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, prediction, pos_label=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc_clf.predict(tf_idf.transform((count_vect.transform([r'document']))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
