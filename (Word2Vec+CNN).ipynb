{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Librarairs\n",
    "import os,json\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "CWD = os.getcwd()  # get current directory path\n",
    "BUG_REPORTS_DIR = os.path.join(CWD,\"bugs\")   # current directory+bugs\n",
    "BUG_REPORTS_LIST = os.listdir(BUG_REPORTS_DIR)  # all the items in 'current_directory+bugs'\n",
    "\n",
    "SENTI_REPORTS_DIR = os.path.join(CWD,\"Sentiments_output\")\n",
    "SENTI_REPORTS_LIST = os.listdir(SENTI_REPORTS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cr/Desktop/bugs\n",
      "-----------------\n",
      "['Core.json', 'Toolkit.json', 'bugzilla.json', 'Calendar.json', 'SeaMonkey.json', 'Camino Graveyard.json', 'MailNews Core.json', 'Firefox.json', 'Thunderbird.json', 'Core Graveyard.json']\n",
      "-----------------\n",
      "/home/cr/Desktop/Sentiments_output\n",
      "-----------------\n",
      "['Camino_Graveyard_sentiments.csv', 'bugzilla_sentiments.csv', 'Firefox_sentiments.csv', 'Core_sentiments.csv', 'MailNews_Core_sentiments.csv', 'SeaMonkey_sentiments.csv', 'Thunderbird_sentiments.csv', 'Toolkit_sentiments.csv', 'Core_Graveyard_sentiments.csv', 'Calendar_sentiments.csv']\n"
     ]
    }
   ],
   "source": [
    "# Load data set for bugs and sentiments\n",
    "\n",
    "print(BUG_REPORTS_DIR)\n",
    "print('-----------------')\n",
    "print(BUG_REPORTS_LIST)\n",
    "print('-----------------')\n",
    "print(SENTI_REPORTS_DIR)\n",
    "print('-----------------')\n",
    "print(SENTI_REPORTS_LIST)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set: Camino_Graveyard_sentiments.csv\n",
      "1291\n",
      "Test Set: bugzilla_sentiments.csv\n",
      "5325\n",
      "Train Set: Firefox_sentiments.csv\n",
      "7369\n",
      "Train Set: Core_sentiments.csv\n",
      "7229\n",
      "Train Set: MailNews_Core_sentiments.csv\n",
      "2797\n",
      "Train Set: SeaMonkey_sentiments.csv\n",
      "8717\n",
      "Train Set: Thunderbird_sentiments.csv\n",
      "8717\n",
      "Train Set: Toolkit_sentiments.csv\n",
      "1900\n",
      "Train Set: Core_Graveyard_sentiments.csv\n",
      "889\n",
      "Train Set: Calendar_sentiments.csv\n",
      "1786\n"
     ]
    }
   ],
   "source": [
    "# Extract data from files for sentiments\n",
    "import csv\n",
    "test_senti = []\n",
    "train_senti=[]\n",
    "import pandas as pd\n",
    "for count, report in enumerate(SENTI_REPORTS_LIST):\n",
    "    \n",
    "    if count == 1:\n",
    "        #add this project to test set\n",
    "        print(f'Test Set: {report}')\n",
    "        with open(os.path.join(SENTI_REPORTS_DIR,report), 'r') as report_file:\n",
    "            temp_data = pd.read_csv(report_file)\n",
    "            print(len(temp_data))\n",
    "            #print(temp_data['neutral'])\n",
    "            test_senti.extend(temp_data.iloc[0:, 1])\n",
    "            \n",
    "    else:\n",
    "        #appent this project to train set\n",
    "        print(f'Train Set: {report}')\n",
    "        with open(os.path.join(SENTI_REPORTS_DIR,report), 'r',encoding='cp850') as report_file:\n",
    "            temp_data = pd.read_csv(report_file)\n",
    "            print(len(temp_data))\n",
    "            #print(temp_data.iloc[0:, 1])\n",
    "            train_senti.extend(temp_data.iloc[0:, 1])\n",
    "            \n",
    "            #print(f\"File:{report} <> Reports: {len(temp_data['bugs'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set: Core.json\n",
      "File:Core.json <> Reports: 7228\n",
      "Train Set: Toolkit.json\n",
      "File:Toolkit.json <> Reports: 1899\n",
      "Train Set: bugzilla.json\n",
      "File:bugzilla.json <> Reports: 5324\n",
      "Train Set: Calendar.json\n",
      "File:Calendar.json <> Reports: 1785\n",
      "Train Set: SeaMonkey.json\n",
      "File:SeaMonkey.json <> Reports: 8716\n",
      "Train Set: Camino Graveyard.json\n",
      "File:Camino Graveyard.json <> Reports: 1290\n",
      "Train Set: MailNews Core.json\n",
      "File:MailNews Core.json <> Reports: 2796\n",
      "Train Set: Firefox.json\n",
      "File:Firefox.json <> Reports: 7368\n",
      "Train Set: Thunderbird.json\n",
      "File:Thunderbird.json <> Reports: 5129\n",
      "Train Set: Core Graveyard.json\n",
      "File:Core Graveyard.json <> Reports: 888\n"
     ]
    }
   ],
   "source": [
    "# Load data for bugs \n",
    "train_set = []\n",
    "test_set = []\n",
    "\n",
    "for count, report in enumerate(BUG_REPORTS_LIST):\n",
    "    if count == 0:\n",
    "        #add this project to test set\n",
    "        print(f'Test Set: {report}')\n",
    "        with open(os.path.join(BUG_REPORTS_DIR,report), 'r',encoding='cp850') as report_file:\n",
    "            temp_data = json.load(report_file)\n",
    "            print(f\"File:{report} <> Reports: {len(temp_data['bugs'])}\")\n",
    "            test_set.extend(temp_data[\"bugs\"])\n",
    "    else:\n",
    "        #appent this project to train set\n",
    "        print(f'Train Set: {report}')\n",
    "        with open(os.path.join(BUG_REPORTS_DIR,report), 'r',encoding='cp850') as report_file:\n",
    "            temp_data = json.load(report_file)\n",
    "            print(f\"File:{report} <> Reports: {len(temp_data['bugs'])}\")\n",
    "            train_set.extend(temp_data[\"bugs\"])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_summary 35195\n",
      "train_label 35195\n",
      "test_summary 7228\n",
      "test_label 7228\n"
     ]
    }
   ],
   "source": [
    "summary_train=[]\n",
    "label_train=[]\n",
    "summary_test=[]\n",
    "label_test=[]\n",
    "for i in train_set:\n",
    "    summary_train.append(i['summary'])\n",
    "    #summary_train=summary_train[0:10]\n",
    "    label_train.append(i['resolution'])\n",
    "print('train_summary',len(summary_train))\n",
    "print('train_label',len(label_train))\n",
    "\n",
    "    #label_train=label_train[0:10]\n",
    "for j in test_set:\n",
    "    summary_test.append(j['summary'])\n",
    "    #summary_test=summary_test[0:10]\n",
    "    label_test.append(j['resolution'])\n",
    "   # label_test=label_test[0:10]\n",
    "print('test_summary',len(summary_test))\n",
    "print('test_label',len(label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def senti_to_int(train_test_senti):\n",
    "    \n",
    "    train_test_sentiments=[]\n",
    "    for x in range(len(train_test_senti)):\n",
    "        if train_test_senti[x]==\"positive\":\n",
    "            train_test_senti[x]=1\n",
    "        elif train_test_senti[x]==\"neutral\":\n",
    "            train_test_senti[x]=1\n",
    "        elif train_senti[x]==\"negative\":\n",
    "            train_test_senti[x]=0\n",
    "        train_test_sentiments.append(train_test_senti[x])\n",
    "    return train_test_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess sentences\n",
    "import re\n",
    "def train_test_summary(summaries_text):\n",
    "    \n",
    "    lemmas=[]\n",
    "    lemmatized_word=[]\n",
    "    train_test_summaries=[]\n",
    "    for count, summaries in enumerate (summaries_text):\n",
    "        text = re.sub(r'[^a-zA-Z\\']', ' ', summaries)\n",
    "        #removes all single characters\n",
    "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "        #substitute multiple spcaces with single space\n",
    "        s_m_s = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "        #removes prefix\n",
    "        r_prefix = re.sub(r'^b\\s+', '', s_m_s)\n",
    "        #lemmas.append(r_prefix)\n",
    "        r_pm = re.sub(r'[^\\w\\s]', '', r_prefix)\n",
    "        step1 = r_pm.replace(\"`` \", '\"').replace(\" ''\", '\"').replace('. . .',  '...')\n",
    "        step2 = step1.replace(\" ( \", \" (\").replace(\" ) \", \") \")\n",
    "        step3 = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", step2)\n",
    "        step4 = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", step3)\n",
    "        step5 = step4.replace(\" '\", \"'\").replace(\" n't\", \"n't\").replace(\n",
    "        \"can not\", \"cannot\")\n",
    "        step6 = step5.replace(\" ` \", \" '\")\n",
    "        step7=step6.strip()\n",
    "        step8=step7.lower()    \n",
    "        lemmas=[]\n",
    "        sentences = nltk.word_tokenize(step7)\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        stem_words=[]\n",
    "        for sentence_words in sentences:\n",
    "            lower=sentence_words.lower()\n",
    "            if lower not in stopwords.words('english'):\n",
    "                lemma = lemmatizer.lemmatize(lower)\n",
    "                stem_words.append(lemma)\n",
    "            untokenized_sentence= ' '.join(stem_words)\n",
    "        train_test_summaries.append(untokenized_sentence)\n",
    "    return train_test_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encocede Labels \n",
    "def train_test_labels(labels):\n",
    "    train_test_label=[]\n",
    "    for x in range(len(labels)):\n",
    "        \n",
    "        \n",
    "        if label_train[x]==\"FIXED\":\n",
    "            label_train[x]=1\n",
    "        elif label_train[x]==\"WONTFIX\":\n",
    "            label_train[x]=0\n",
    "        elif label_train[x]==\"INVALID\":\n",
    "            label_train[x]=0\n",
    "        elif label_train[x]==\"INCOMPLETE\":\n",
    "            label_train[x]=0\n",
    "        elif label_train[x]==\"WONTFIX\":\n",
    "            label_train[x]=0\n",
    "        elif label_train[x]==\"WORKSFORME\":\n",
    "            label_train[x]=0\n",
    "        elif label_train[x]==\"EXPIRED\":\n",
    "            label_train[x]=0\n",
    "        elif label_train[x]==\"DUPLICATE\":\n",
    "            label_train[x]=0\n",
    "        elif label_train[x]==\"\":\n",
    "            label_train[x]=0\n",
    "        train_test_label.append(label_train[x])\n",
    "        \n",
    "    \n",
    "    \n",
    "    #print('train_lbel', len(train_label))\n",
    "    return train_test_label\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels= train_test_labels(label_train)\n",
    "test_labels=train_test_labels(label_test)\n",
    "train_summary=train_test_summary(summary_train)\n",
    "test_summary=train_test_summary(summary_test)\n",
    "\n",
    "train_sentiment=senti_to_int(train_senti)\n",
    "test_sentiment=senti_to_int(test_senti)\n",
    "\n",
    "# Sentiments \n",
    "\n",
    "train_senti=np.array(train_sentiment)\n",
    "test_senti=np.array(test_sentiment)\n",
    "#train_sentiments.shape\n",
    "train_sentiments=train_senti.reshape(len(train_senti),1)\n",
    "test_sentiments=test_senti.reshape(len(test_senti),1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training summary Length:35195\n",
      "Testing summary Length:7228\n",
      "Training Labels Length:35195\n",
      "Testing Labels Length:7228\n",
      "Training sentimetns Length:40695\n",
      "Testing sentiments Length:5325\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training summary Length:{len(train_summary)}\")\n",
    "print(f\"Testing summary Length:{len(test_summary)}\")\n",
    "print(f\"Training Labels Length:{len(train_labels)}\")\n",
    "print(f\"Testing Labels Length:{len(test_labels)}\")\n",
    "print(f\"Training sentimetns Length:{len(train_sentiment)}\")\n",
    "print(f\"Testing sentiments Length:{len(test_sentiment)}\")\n",
    "\n",
    "\n",
    "# Use \n",
    "#     train_labels\n",
    "#     test_labels\n",
    "\n",
    "# Use \n",
    "#     train_summary\n",
    "#     test_summary\n",
    "\n",
    "# Use \n",
    "#     train_sentiments\n",
    "#     test_sentiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10612 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "NUM_WORDS=1000\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',\n",
    "                      lower=True)\n",
    "tokenizer.fit_on_texts(train_summary)\n",
    "sequences_train = tokenizer.texts_to_sequences(train_summary)\n",
    "sequences_valid=tokenizer.texts_to_sequences(test_summary)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X train and X validation tensor: (35195, 28) (7228, 28)\n",
      "Shape of label train and validation tensor: (35195, 2) (7228, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train = pad_sequences(sequences_train)\n",
    "X_val = pad_sequences(sequences_valid,maxlen=X_train.shape[1])\n",
    "y_train = to_categorical(np.asarray(train_labels))\n",
    "y_val = to_categorical(np.asarray(test_labels))\n",
    "print('Shape of X train and X validation tensor:', X_train.shape,X_val.shape)\n",
    "print('Shape of label train and validation tensor:', y_train.shape,y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0902 00:01:05.127656 140186585446208 smart_open_lib.py:378] this function is deprecated, use smart_open.open instead\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format('/home/cr/Desktop/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "EMBEDDING_DIM=300\n",
    "vocabulary_size=min(len(word_index)+1,NUM_WORDS)\n",
    "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i>=NUM_WORDS:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
    "\n",
    "del(word_vectors)\n",
    "\n",
    "from keras.layers import Embedding\n",
    "embedding_layer = Embedding(vocabulary_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0902 00:01:45.936730 140186585446208 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0902 00:01:45.940925 140186585446208 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0902 00:01:45.960899 140186585446208 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0902 00:01:45.962442 140186585446208 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "sequence_length = X_train.shape[1]\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 100\n",
    "drop = 0.5\n",
    "\n",
    "\n",
    "\n",
    "inputs = Input(shape=(sequence_length,))\n",
    "embedding = embedding_layer(inputs)\n",
    "reshape = Reshape((sequence_length,EMBEDDING_DIM,1))(embedding)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, (filter_sizes[0], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "conv_1 = Conv2D(num_filters, (filter_sizes[1], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "conv_2 = Conv2D(num_filters, (filter_sizes[2], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "\n",
    "maxpool_0 = MaxPooling2D((sequence_length - filter_sizes[0] + 1, 1), strides=(1,1))(conv_0)\n",
    "maxpool_1 = MaxPooling2D((sequence_length - filter_sizes[1] + 1, 1), strides=(1,1))(conv_1)\n",
    "maxpool_2 = MaxPooling2D((sequence_length - filter_sizes[2] + 1, 1), strides=(1,1))(conv_2)\n",
    "\n",
    "merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\n",
    "flatten = Flatten()(merged_tensor)\n",
    "reshape = Reshape((3*num_filters,))(flatten)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(units=3, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
    "\n",
    "# this creates a model that includes\n",
    "model = Model(inputs, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Adam' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-021b3a35ceca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0madam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m model.compile(loss='categorical_crossentropy',\n\u001b[1;32m      4\u001b[0m               \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madam\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m               metrics=['acc'])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Adam' is not defined"
     ]
    }
   ],
   "source": [
    "adam = Adam(lr=1e-3)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['acc'])\n",
    "callbacks = [EarlyStopping(monitor='val_loss')]\n",
    "model.fit(X_train, y_train, batch_size=1000, epochs=10, verbose=1, validation_data=(X_val, y_val),\n",
    "         callbacks=callbacks)  # starts training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_test=tokenizer.texts_to_sequences(test_data.text)\n",
    "X_test = pad_sequences(sequences_test,maxlen=X_train.shape[1])\n",
    "y_pred=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_class = (y_pred > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true=test_labels, y_pred=yhat_class, digits=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
