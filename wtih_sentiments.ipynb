{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,json\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "#import DataFrame as df \n",
    "CWD = os.getcwd()\n",
    "BUG_REPORTS_DIR = os.path.join(CWD,\"Sentiments_output\")\n",
    "BUG_REPORTS_LIST = os.listdir(BUG_REPORTS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cr/Desktop/Sentiments_output\n",
      "-----------------\n",
      "['Camino_Graveyard_sentiments.csv', 'bugzilla_sentiments.csv', 'Firefox_sentiments.csv', 'Core_sentiments.csv', 'MailNews_Core_sentiments.csv', 'SeaMonkey_sentiments.csv', 'Thunderbird_sentiments.csv', 'Toolkit_sentiments.csv', 'Core_Graveyard_sentiments.csv', 'Calendar_sentiments.csv']\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "print(BUG_REPORTS_DIR)\n",
    "print('-----------------')\n",
    "print(BUG_REPORTS_LIST)\n",
    "print('-----------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set: Camino_Graveyard_sentiments.csv\n",
      "1291\n",
      "Train Set: bugzilla_sentiments.csv\n",
      "5325\n",
      "Train Set: Firefox_sentiments.csv\n",
      "7369\n",
      "Train Set: Core_sentiments.csv\n",
      "7229\n",
      "Train Set: MailNews_Core_sentiments.csv\n",
      "2797\n",
      "Train Set: SeaMonkey_sentiments.csv\n",
      "8717\n",
      "Train Set: Thunderbird_sentiments.csv\n",
      "8717\n",
      "Train Set: Toolkit_sentiments.csv\n",
      "1900\n",
      "Train Set: Core_Graveyard_sentiments.csv\n",
      "889\n",
      "Train Set: Calendar_sentiments.csv\n",
      "1786\n"
     ]
    }
   ],
   "source": [
    "train_senti = []\n",
    "test_senti = []\n",
    "\n",
    "for count, report in enumerate(BUG_REPORTS_LIST):\n",
    "    if count == 0:\n",
    "        #add this project to test set\n",
    "        print(f'Test Set: {report}')\n",
    "        with open(os.path.join(BUG_REPORTS_DIR,report), 'r',encoding='cp850') as report_file:\n",
    "            temp_data = pd.read_csv(report_file)\n",
    "            #print(temp_data)\n",
    "            predictions=temp_data.iloc[0:, 1]\n",
    "            #predictions=df.temp_data.iloc[0:, 1]\n",
    "           \n",
    "            print(len(predictions))\n",
    "            test_senti.extend(predictions)\n",
    "                \n",
    "            \n",
    "            #print(type(df))\n",
    "            #train_set.append()\n",
    "           # print(f\"File:{report} <> Reports: {len(df)}\")\n",
    "            \n",
    "    else:\n",
    "        #appent this project to train set\n",
    "        print(f'Train Set: {report}')\n",
    "        with open(os.path.join(BUG_REPORTS_DIR,report), 'r',encoding='cp850') as report_file:\n",
    "            temp_data = pd.read_csv(report_file)\n",
    "            \n",
    "            predictions=temp_data.iloc[0:, 1]\n",
    "            print(len(predictions))\n",
    "            train_senti.extend(predictions)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1291\n"
     ]
    }
   ],
   "source": [
    "print(len(test_senti))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46020\n"
     ]
    }
   ],
   "source": [
    "print(len(train_senti+test_senti))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44729\n"
     ]
    }
   ],
   "source": [
    "print(len(train_senti))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_sentiments 44729\n"
     ]
    }
   ],
   "source": [
    "train_sentiments=[]\n",
    "for x in range(len(train_senti)):\n",
    "    \n",
    "    \n",
    "    if train_senti[x]==\"positive\":\n",
    "        train_senti[x]=1\n",
    "    elif train_senti[x]==\"neutral\":\n",
    "        train_senti[x]=0\n",
    "    elif train_senti[x]==\"negative\":\n",
    "        train_senti[x]=0\n",
    "    train_sentiments.append(train_senti[x])\n",
    "print('train_sentiments', len(train_sentiments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#print(train_sentiments.toarray)\n",
    "import numpy as np\n",
    "train_sentiments=np.array(train_sentiments)\n",
    "print(type(train_sentiments))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentiments.shape\n",
    "train_sentiments=train_sentiments.reshape(len(train_sentiments),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44729, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_sentiments.shape) \n",
    "\n",
    "\n",
    "#####   train_sentiments ######\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_sentiments 1291\n"
     ]
    }
   ],
   "source": [
    "test_sentiments=[]\n",
    "for x in range(len(test_senti)):\n",
    "    \n",
    "    \n",
    "    if test_senti[x]==\"positive\":\n",
    "        test_senti[x]=1\n",
    "    elif test_senti[x]==\"neutral\":\n",
    "        test_senti[x]=0\n",
    "    elif test_senti[x]==\"negative\":\n",
    "        test_senti[x]=0\n",
    "    test_sentiments.append(test_senti[x])\n",
    "print('test_sentiments', len(test_sentiments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentiments=np.array(test_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1785,)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentiments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentiments=test_sentiments.reshape(len(test_sentiments),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1785, 1)\n"
     ]
    }
   ],
   "source": [
    "print(test_sentiments.shape)      \n",
    "\n",
    "#########   test_sentiments   ########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,json\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "CWD = os.getcwd()\n",
    "BUG_REPORTS_DIR = os.path.join(CWD,\"bugs\")\n",
    "BUG_REPORTS_LIST = os.listdir(BUG_REPORTS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cr/Desktop/bugs\n",
      "-----------------\n",
      "['Core.json', 'Toolkit.json', 'bugzilla.json', 'Calendar.json', 'SeaMonkey.json', 'Camino Graveyard.json', 'MailNews Core.json', 'Firefox.json', 'Thunderbird.json', 'Core Graveyard.json']\n",
      "-----------------\n",
      "Train Set: Core.json\n",
      "File:Core.json <> Reports: 7228\n",
      "Train Set: Toolkit.json\n",
      "File:Toolkit.json <> Reports: 1899\n",
      "Train Set: bugzilla.json\n",
      "File:bugzilla.json <> Reports: 5324\n",
      "Train Set: Calendar.json\n",
      "File:Calendar.json <> Reports: 1785\n",
      "Train Set: SeaMonkey.json\n",
      "File:SeaMonkey.json <> Reports: 8716\n",
      "Test Set: Camino Graveyard.json\n",
      "File:Camino Graveyard.json <> Reports: 1290\n",
      "Train Set: MailNews Core.json\n",
      "File:MailNews Core.json <> Reports: 2796\n",
      "Train Set: Firefox.json\n",
      "File:Firefox.json <> Reports: 7368\n",
      "Train Set: Thunderbird.json\n",
      "File:Thunderbird.json <> Reports: 5129\n",
      "Train Set: Core Graveyard.json\n",
      "File:Core Graveyard.json <> Reports: 888\n"
     ]
    }
   ],
   "source": [
    "print(BUG_REPORTS_DIR)\n",
    "print('-----------------')\n",
    "print(BUG_REPORTS_LIST)\n",
    "print('-----------------')\n",
    "\n",
    "\n",
    "train_set = []\n",
    "test_set = []\n",
    "\n",
    "for count, report in enumerate(BUG_REPORTS_LIST):\n",
    "    if count == 5:\n",
    "        #add this project to test set\n",
    "        print(f'Test Set: {report}')\n",
    "        with open(os.path.join(BUG_REPORTS_DIR,report), 'r',encoding='cp850') as report_file:\n",
    "            temp_data = json.load(report_file)\n",
    "            print(f\"File:{report} <> Reports: {len(temp_data['bugs'])}\")\n",
    "            test_set.extend(temp_data[\"bugs\"])\n",
    "    else:\n",
    "        #appent this project to train set\n",
    "        print(f'Train Set: {report}')\n",
    "        with open(os.path.join(BUG_REPORTS_DIR,report), 'r',encoding='cp850') as report_file:\n",
    "            temp_data = json.load(report_file)\n",
    "            print(f\"File:{report} <> Reports: {len(temp_data['bugs'])}\")\n",
    "            train_set.extend(temp_data[\"bugs\"])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train len: 41133\n",
      "Test len: 1290\n",
      "Total len: 42423\n"
     ]
    }
   ],
   "source": [
    "print(f'Train len: {len(train_set)}')\n",
    "print(f'Test len: {len(test_set)}')\n",
    "print(f'Total len: {len(train_set)+len(test_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "lemmas=[]\n",
    "lemmatized_word=[]\n",
    "train_summary=[]\n",
    "for count, summaries in enumerate (summary_train):\n",
    "    text = re.sub(r'[^a-zA-Z\\']', ' ', summaries)\n",
    "    #removes all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "    #substitute multiple spcaces with single space\n",
    "    s_m_s = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    #removes prefix\n",
    "    r_prefix = re.sub(r'^b\\s+', '', s_m_s)\n",
    "    #lemmas.append(r_prefix)\n",
    "    r_pm = re.sub(r'[^\\w\\s]', '', r_prefix)\n",
    "    step1 = r_pm.replace(\"`` \", '\"').replace(\" ''\", '\"').replace('. . .',  '...')\n",
    "    step2 = step1.replace(\" ( \", \" (\").replace(\" ) \", \") \")\n",
    "    step3 = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", step2)\n",
    "    step4 = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", step3)\n",
    "    step5 = step4.replace(\" '\", \"'\").replace(\" n't\", \"n't\").replace(\n",
    "        \"can not\", \"cannot\")\n",
    "    step6 = step5.replace(\" ` \", \" '\")\n",
    "    step7=step6.strip()\n",
    "    step8=step7.lower()    \n",
    "    lemmas=[]\n",
    "    sentences = nltk.word_tokenize(step7)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stem_words=[]\n",
    "    for sentence_words in sentences:\n",
    "        lower=sentence_words.lower()\n",
    "        if lower not in stopwords.words('english'):\n",
    "            lemma = lemmatizer.lemmatize(lower)\n",
    "            stem_words.append(lemma)\n",
    "            \n",
    "    untokenized_sentence= ' '.join(stem_words)\n",
    "    train_summary.append(untokenized_sentence)\n",
    "    \n",
    "print('train_summary',len(train_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas=[]\n",
    "lemmatized_word=[]\n",
    "test_summary=[]\n",
    "for count, summaries in enumerate (summary_test):\n",
    "    text = re.sub(r'[^a-zA-Z\\']', ' ', summaries)\n",
    "    #removes all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "    #substitute multiple spcaces with single space\n",
    "    s_m_s = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    #removes prefix\n",
    "    r_prefix = re.sub(r'^b\\s+', '', s_m_s)\n",
    "    #lemmas.append(r_prefix)\n",
    "    r_pm = re.sub(r'[^\\w\\s]', '', r_prefix)\n",
    "    step1 = r_pm.replace(\"`` \", '\"').replace(\" ''\", '\"').replace('. . .',  '...')\n",
    "    step2 = step1.replace(\" ( \", \" (\").replace(\" ) \", \") \")\n",
    "    step3 = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", step2)\n",
    "    step4 = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", step3)\n",
    "    step5 = step4.replace(\" '\", \"'\").replace(\" n't\", \"n't\").replace(\n",
    "        \"can not\", \"cannot\")\n",
    "    step6 = step5.replace(\" ` \", \" '\")\n",
    "    step7=step6.strip()\n",
    "    step8=step7.lower()    \n",
    "    lemmas=[]\n",
    "    sentences = nltk.word_tokenize(step7)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stem_words=[]\n",
    "    for sentence_words in sentences:\n",
    "        lower=sentence_words.lower()\n",
    "        if lower not in stopwords.words('english'):\n",
    "            lemma = lemmatizer.lemmatize(lower)\n",
    "            stem_words.append(lemma)\n",
    "            \n",
    "    untokenized_sentence= ' '.join(stem_words)\n",
    "    test_summary.append(untokenized_sentence)\n",
    "    \n",
    "print('test_summary', len(test_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label=[]\n",
    "for x in range(len(label_train)):\n",
    "    \n",
    "    if label_train[x]==\"FIXED\":\n",
    "        label_train[x]=1\n",
    "    elif label_train[x]==\"WONTFIX\":\n",
    "        label_train[x]=0\n",
    "    elif label_train[x]==\"INVALID\":\n",
    "        label_train[x]=0\n",
    "    elif label_train[x]==\"INCOMPLETE\":\n",
    "        label_train[x]=0\n",
    "    elif label_train[x]==\"WONTFIX\":\n",
    "        label_train[x]=0\n",
    "    elif label_train[x]==\"WORKSFORME\":\n",
    "        label_train[x]=0\n",
    "    elif label_train[x]==\"EXPIRED\":\n",
    "        label_train[x]=0\n",
    "    elif label_train[x]==\"DUPLICATE\":\n",
    "        label_train[x]=0\n",
    "    elif label_train[x]==\"\":\n",
    "        \n",
    "        label_train[x]=0\n",
    "    train_label.append(label_train[x])\n",
    "print('train_lbel', len(train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label=[]\n",
    "for x in range(len(label_test)):\n",
    "    if label_test[x]==\"FIXED\":\n",
    "        label_test[x]=1\n",
    "    elif label_test[x]==\"WONTFIX\":\n",
    "        label_test[x]=0\n",
    "    elif label_test[x]==\"INVALID\":\n",
    "        label_test[x]=0\n",
    "    elif label_test[x]==\"INCOMPLETE\":\n",
    "        label_test[x]=0\n",
    "    elif label_test[x]==\"WONTFIX\":\n",
    "        label_test[x]=0\n",
    "    elif label_test[x]==\"WORKSFORME\":\n",
    "        label_test[x]=0\n",
    "    elif label_test[x]==\"EXPIRED\":\n",
    "        label_test[x]=0\n",
    "    elif label_test[x]==\"DUPLICATE\":\n",
    "        label_test[x]=0\n",
    "    elif label_test[x]==\"\":\n",
    "        label_test[x]=0\n",
    "    test_label.append(label_test[x])\n",
    "print(\"test_labels:\",len(test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_summary)\n",
    "len(test_summary)\n",
    "len(train_label)\n",
    "len(test_label)\n",
    "len(test_sentiments)\n",
    "len(train_sentiments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
